---
title: "LAX_passenger_prediction_with_Box_Jenkins"
author: "Edgar Julien, Antoine Settelen, Simon Weiss"
date: "2020-10-20"
output:
  html_document :
   keep_md: true
---

# Navigation {.tabset .tabset-fade .tabset-pills}

Suggested outline : 


1. Description of the time series

2. Stationarity: Is your series stationary? Include graphs and tables to justify your analysis (acf, pacf, graph of the series). Explain how you transformed the data in order to reach stationarity (log transformation, differentiation with different orders).

3. Apply Box-Jenkins methodology to build an ARMA process on the stationary data. a. Identify the orders p and q using the ACF and PACF. b. Comment the significance of the coefficients. If necessary, modify the model until all the coefficients are significant. c. Residual diagnostic: check for the normality, the non autocorrelation assumption and the homoscedasticity of the residuals. d. Comment the information criteria values (AIC, SBC) to select the best model.

4. Validate the model with an in-sample and out-of-sample analysis and do a forecast for the next three periods.

## 1. Description of the time series

```{r}
library(magrittr)
library(data.table)
library(forecast)
library(tseries)
library(TSA)
```

### 1.1 Load the data
```{r}
#TlseTraf <- read.csv2('TlseAirport2019.csv')
#View(TlseTraf)
#traf <- ts(TlseTraf[,3], start=c(1982,1), frequency=12)

Lax_init <- read.csv("data/los-angeles-international-airport-passenger-traffic-by-terminal.csv")
Lax_init[,2]<-Lax_init[,2] %>% as.Date()
Lax_init$year<-as.numeric(format(Lax_init[,2],'%Y'))
Lax_init$months<-months(Lax_init[,2])
Lax_init %>% setDT

Lax_data=Lax_init[, lapply(.SD, sum, na.rm=TRUE), by=list(year,months),.SDcols=c("Passenger_Count")]
#verify results

sum(Lax_init[months=="janvier"&year==2006][,6])
Lax_data[1]

Lax_data %>% View

```

```{r}
traf <- ts(Lax_data[,3], start=c(2006,1), frequency=12)
plot.ts(traf) #presence d'un cycle, d'une tendance, et l'amplitude du cycle croit avec le temps
#plot.ts(traf, xlim=(c(2000,2010)))

```

```{r}
acf(ts(traf, frequency=1))
pacf(ts(traf, frequency=1))
```

#Logarithm transformation
We perform a logarithmic transformation to remove the cycle amplification
```{r}
ltraf <- log(traf)
```
CSQ: the range of the cycle has become stable
```{r}
plot.ts(ltraf)
```

Still a presence of a trend and a seasonal component
This trend seems to be deterministic, linear and non-stochastic (but we will use a sotchastic modelisation for the trend using integration
within ARIMA)
The serie is always a non-stationary series 
```{r}
acf(ts(ltraf, frequency=1))
pacf(ts(ltraf, frequency=1))
```
#1st order diff: remove the trend
```{r}
dltraf_1 <- diff(ltraf, 1)
plot(dltraf_1)
acf(ts(dltraf_1, frequency=1))
```
the trend component seems to have disappeared, not the seasonality
```{r}
pacf(ts(dltraf_1, frequency=1), lag.max = 60)
```

A linear regression of degree 1 seems to be sufficient.
However, the lm1 model cannot be used to make predictions.
Because there is still self-correction in the residuals. Let's refine the model by modeling the residuals.

We will use a stochastic modelisation for the trend using integration within ARIMA
#Now we need to look at the seasonal component 

```{r}
dltraf_12 <- diff(dltraf_1, 12)
library(tseries)
kpss.test(dltraf_12) #stationnary, we accept H0 and we choose as seasonality s=12


#dltraf_6 <- diff(dltraf_1, 6)
#plot(dltraf_6)
#kpss.test(dltraf_6)

```
#now we have found a series which is stationary. We can move the identification step
```{r}
plot(dltraf_12)
par(mfrow=c(1,1))
acf(ts(dltraf_12, frequency=1), lag.max = 40)
pacf(ts(dltraf_12, frequency=1), lag.max = 40)
```
## STEP 2 : Identification (p,d,q,P,D,Q)

#first estimation 
q=1 #MA because for k>(q=1), acf(ts) = 0 (taking ac count of the saisonality)
p=2 #AR because for k>(q=2), pacf(ts) = 0 (taking ac count of the saisonality)
d=1 #we removed trend by taking the diff or order 1
Q=1 #MA because some significant coefficients in the ACF remain at order 12 (but not at order 24)
P=2 #AR because some significant coefficients in the PACF remain at order 12 & at order 24 
D=1 #(seasonality diff) we removed seasonality diff by taking the diff or order 1

```{r}
mod1 <- arima(ltraf, c(2, 1, 1),seasonal = list(order = c(1, 1, 1), period = 12), method='ML')
mod1
```

```{r}
fit1 <- fitted(mod1)
plot.ts(cbind(ltraf,fit1),plot.type='single',col=c('black','red'))
```

```{r}
mod1$coef
mod1$var.coef
```
```{r}
tstat <- mod1$coef/sqrt(diag(mod1$var.coef))
tstat  
```
```{r}
pvalue <- 2*(1-pnorm(abs(tstat)))
pvalue 

# pvalue<5% for sma1.
# fortunately pvalue<5% for sma1. The coefficients associated to the sma1 is significant 
# the coefficients associated to the other coefficients are not significant because pvalue>5% for them

# we need to remove them, re-estimate the model without them to accurate the model
```
###  First, we choose to remove ar1 & ar2 

```{r}
mod1 <- arima(ltraf, c(0, 1, 1),seasonal = list(order = c(1, 1, 1), period = 12), method='ML')
mod1
```

### Check p-value 2

```{r}
mod1$coef
mod1$var.coef
```
```{r}
tstat <- mod1$coef/sqrt(diag(mod1$var.coef))
tstat  
```
```{r}
pvalue <- 2*(1-pnorm(abs(tstat)))
pvalue 

# pvalue<5% for sma1. The coefficients assoc

```

### We remove sar1

```{r}
mod1 <- arima(ltraf, c(0, 1, 1),seasonal = list(order = c(0, 1, 1), period = 12), method='ML')
mod1
```

### We check p-value 3
```{r}
mod1$coef
mod1$var.coef
```
```{r}
tstat <- mod1$coef/sqrt(diag(mod1$var.coef))
tstat  
```

```{r}
pvalue <- 2*(1-pnorm(abs(tstat)))
pvalue
```
We check with auto arima final coefficient 
#Test auto.arima
```{r}
auto.arima(ltraf,stationary =FALSE, seasonal = TRUE,trace=TRUE)
mod1<- arima(ltraf, c(0, 1, 1),seasonal = list(order = c(0, 1, 1), period = 12), method='ML')
mod1
fit1 <- fitted(mod1)
plot.ts(cbind(ltraf,fit1),plot.type='single',col=c('black','red'))
```
```{r}
mod1$coef
mod1$var.coef

tstat <- mod1$coef/sqrt(diag(mod1$var.coef))
tstat 
pvalue <- 2*(1-pnorm(abs(tstat)))
pvalue
```
We have the same coef
We follow to the residuals analysis

## Residuals analysis ## 
Difference between fit data and raw data
every time there is a pic explain that the model has not fitted well the data

```{r}
res1<- mod1$residuals
plot(res1)
```
What we expect is that residuals are close to 0 as possible
We want to check the white noise assumption and the normality of the residuals (gaussian WN)


#1. White noise assumption
Autocorrelation of the residuals (White noise assumption) 
```{r}
acf(ts(res1, frequency=1))
```
what we expect is that all the coefficients are none significant, that is the deal with white noise
All coefficient seem to be none significant

```{r}
pacf(ts(res1, frequency=1),lag.max = 60)
```
what i expect is that all the coefficients are none significant, that is the deal with white noise
it is not perfect but what would be like an issue if the first or the second autocorellation coefficient 
would have been significant
here, 1 significant coefficient at lag 4 

# Ljung-box test to check for the significance of ACF
```{r}
Box.test(res1,lag=20,type="Ljung-Box")
```
pvalue = 59% larger than 5% so we accept H0: white noise assumption
CCL => We have a white noise


#2. Normality assumption

Normality assumption
standardized residuals (residuals divided by the standard deviation)
```{r}
res_stand <- res1/sqrt(mod1$sigma2)
summary(res_stand)
```
if the normality assumption is satisfied, the standardized residuals should 
lie in between -2 and 2 (with 95% of chance)

```{r}
plot(res_stand) 
abline(a=2,b=0,col="red")
abline(a=-2,b=0,col="red")
```
It should be 95% of chance but here obviously there are some pics around 2007, 2009, 2011, 2014 
(we need to add dummy variables to fit them)
We can identify outliers in the dataset
it corresponds to the lowest values of the residuals

but, before doing that, we need to check if it is necessary by testing the normality of the residuals

#######
#QQ-plot
#######
```{r}
qqnorm(res1)
qqline(res1)
```

```{r}
#######
#Shapiro-test
#######
shapiro.test(res1)
library(tseries)
jarque.bera.test(res1)
```
pvalue <<<< 5%, we reject normality (H0 : normal, H1 non normal)
We need to add a dummy in the model (second option)



constance variance
with a plot
if the variance is constant, the variance should not depend on the difference of the date
(should not depend on t)
# Homoscedaticity or heteroscedaticity here ? 
```{r}
sq.res1 <- (res1)^2
acf(ts(sq.res1,frequency=1),lag.max = 60)
```
```{r}
#library(TSA)
Htest <- McLeod.Li.test(mod1, plot=FALSE)
Htest
```

### Dummy Variable
SARIMAX 

#Identify dummies

```{r}
plot(res_stand) 
abline(a=2,b=0,col="red")
abline(a=-2,b=0,col="red")
```
We can identify outliers in our dataset
2009 : 1 min , 2011 : 1 max , 2014 : 1 min and max

```{r}
min(res_stand)
max(res_stand)
```
We decide to cut at [-3,3]


```{r}
out <- which(res_stand < -3 | res_stand > 3)
out

#split all outliers into 4 groups to create 4 dummies 

out1 <- out[1]
out1

out2 <- out[2]
out2

out3 <- out[3]
out3

out4 <- out[4]
out4


```

It corresponds to the observations 35, 65, 100,11 

With the zoo package we associate with this observation the date. 
```{r}
library(zoo)
```

```{r}
index(res_stand)[out1] #date of the outliers novembre 2008, Mai 2011, Mars 2014, Avril 2014 
res_stand[out1] # value of the outlier. 
```

# We create the dummmies
#We split all outliers into 4 groups to create 4 dummies 
```{r}
#first dummy
Lax_data$dum_1 <- 0
Lax_data$dum_1[out1] <- 1

length(Lax_data$dum_1)

#second dummy
Lax_data$dum_2 <- 0
Lax_data$dum_2[out2] <- 1

#third dummy
Lax_data$dum_3 <- 0
Lax_data$dum_3[out3] <- 1

#fourth dummy
Lax_data$dum_4 <- 0
Lax_data$dum_4[out4] <- 1

```


```{r}
mod2 <- arima(ltraf, c(0, 1, 1),seasonal = list(order = c(0, 1, 1), period = 12), 
              method='ML',xreg=cbind(unlist(Lax_data$dum_1), 
                                     unlist(Lax_data$dum_2),
                                     unlist(Lax_data$dum_3),
                                     unlist(Lax_data$dum_4)
                                     ))

ncol(cbind(unlist(Lax_data$dum_1), unlist(Lax_data$dum_2), unlist(Lax_data$dum_3), unlist(Lax_data$dum_4)))

#mod2 <- arima(ltraf, c(0, 1, 1),seasonal = list(order = c(0, 1, 1), period = 12), 
#              method='ML', xreg=unlist(Lax_data$dum_1))

mod2
mod2$residuals
```

xreg : additional variable I want to take into account

By adding xreg, we will be able to control the misfit at the years written below. We have even gain small AIC ! 


Plot of the fitted value...
```{r}
fit2 <- fitted(mod2)
plot.ts(cbind(ltraf,fit2),plot.type='single',col=c('black','red'))
```
### Residuals analysis

```{r}
res2<- mod2$residuals
plot(res2)
```
# what we expect is that residuals are close to 0 as possible
```{r}
res_stand_2 <- res2/sqrt(mod2$sigma2)
summary(res_stand_2)

```
```{r}
plot(res_stand_2) 
abline(a=2,b=0,col="red")
abline(a=-2,b=0,col="red")
```
```{r}
qqnorm(res2)
qqline(res2)
```

```{r}
shapiro.test(res2)
jarque.bera.test(res2)
```
with our dummies, now the residuals follow a normal distribution.
It is always a white noise ? 

```{r}
# Ljung-box test to check for the significance of ACF
Box.test(res2,lag=20,type="Ljung-Box")
#pvalue = 59% larger than 5% so we accept H0: white noise assumption
```
CCL => We have a gaussian white noise now. 


#Recheck for Heteroscedasticity 

```{r}

acf(ts(res2, frequency=1))
pacf(ts(res2, frequency=1),lag.max = 60)

```
No, it seems that there is no more significant coefficients which would say that we have homoscedaticity (don't need to use garch model)

# McLeod.Li.test
```{r}
Htest <- McLeod.Li.test(mod2, plot=FALSE)
Htest
```
H0 is always accepted : homoscedaticity here ! 

# Step4: Prediction
Quality of the fit
```{r}
cb80 <- mod2$sigma2^.5*qnorm(0.9)
plot(cbind(ltraf,fit2-cb80,fit2+cb80),plot.type='single',lty=c(1,2,2))
```

# Proportion of points in the confidence bound
```{r}
indi <- (ltraf-(fit2-cb80))>0&(fit2+cb80-ltraf)>0
prop <- 100*sum(indi)/length(indi)
prop
```
Here, the prop >80%, then the fit is considered good.

# Prediction 
```{r}
?predict

trick <- matrix(rep(0, 12), ncol=4)

length(trick)

mod2

pred <- predict(mod2, n.ahead = 12, newxreg = trick)

#For forecasting you need to supply new values of your regressor, which apparently you do not.
#newxreg should be a matrix that contains values of the regressors that you want predictions for. 
#So in my case, my regressors were all 1 or 0 (coded variables) to specify a particular month. So what I did is I created a matrix of 0's and 1's to be used as my newxreg. What I did is I defined a matrix mx, and then in the predict function I set newxreg=mx. I made sure that the number of rows of mx>= number of rows of n.ahead.

ts.plot(traf,2.718^pred$pred, log = "y", lty = c(1,3))
time(traf)
ts.plot(traf,xlim=c(2010,2020.500))
lines(2.718^(pred$pred), col="red")
lines(2.718^(pred$pred-1.96*pred$se),col=4,lty=2)
lines(2.718^(pred$pred+1.96*pred$se),col=4,lty=2)

ts.plot(traf,2.718^pred$pred, log = "y", lty = c(1,3))
time(traf)


```




