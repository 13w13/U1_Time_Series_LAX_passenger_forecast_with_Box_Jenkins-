---
title: "LAX_passenger_prediction_with_Box_Jenkins"
author: "Edgar Julien, Antoine Settelen, Simon Weiss"
date: "2020-10-20"
output:
  html_document :
   keep_md: true
---

# Navigation {.tabset .tabset-fade .tabset-pills}

Suggested outline : 


1. Description of the time series

2. Stationarity: Is your series stationary? Include graphs and tables to justify your analysis (acf, pacf, graph of the series). Explain how you transformed the data in order to reach stationarity (log transformation, differentiation with different orders).

3. Apply Box-Jenkins methodology to build an ARMA process on the stationary data. a. Identify the orders p and q using the ACF and PACF. b. Comment the significance of the coefficients. If necessary, modify the model until all the coefficients are significant. c. Residual diagnostic: check for the normality, the non autocorrelation assumption and the homoscedasticity of the residuals. d. Comment the information criteria values (AIC, SBC) to select the best model.

4. Validate the model with an in-sample and out-of-sample analysis and do a forecast for the next three periods.

## 1. Description of the time series

```{r}
library(magrittr)
library(data.table)
library(forecast)
library(TSA)
```

### 1.1 Load the data
```{r}
#TlseTraf <- read.csv2('TlseAirport2019.csv')
#View(TlseTraf)
#traf <- ts(TlseTraf[,3], start=c(1982,1), frequency=12)

Lax_init <- read.csv("data/los-angeles-international-airport-passenger-traffic-by-terminal.csv")
Lax_init[,2]<-Lax_init[,2] %>% as.Date()
Lax_init$year<-as.numeric(format(Lax_init[,2],'%Y'))
Lax_init$months<-months(Lax_init[,2])
Lax_init %>% setDT

Lax_data=Lax_init[, lapply(.SD, sum, na.rm=TRUE), by=list(year,months),.SDcols=c("Passenger_Count")]
#verify results

sum(Lax_init[months=="janvier"&year==2006][,6])
Lax_data[1]

```

```{r}
traf <- ts(Lax_data[,3], start=c(2006,1), frequency=12)
plot.ts(traf) #presence d'un cycle, d'une tendance, et l'amplitude du cycle croit avec le temps
#plot.ts(traf, xlim=(c(2000,2010)))

```

```{r}
acf(ts(traf, frequency=1))
pacf(ts(traf, frequency=1))
```


#We perform a logarithmic transformation
```{r}
ltraf <- log(traf)
```
#CSQ: the range of the cycle has become stable
```{r}
plot.ts(ltraf)
```

#Still the presence of a trend and a seasonal component
#This trend seems to be deterministic, linear and non-stochastic
#The serie is non-stationary series
```{r}
acf(ts(ltraf, frequency=1))
pacf(ts(ltraf, frequency=1))
```

#1st order diff: remove the trend
```{r}
dltraf_1 <- diff(ltraf, 1)
plot(dltraf_1)
acf(ts(dltraf_1, frequency=1))
```
#the trend component seems to have disappeared, not the seasonality
```{r}
pacf(ts(dltraf_1, frequency=1), lag.max = 60)
```

#A linear regression of degree 1 seems to be sufficient. 
#However, the lm1 model cannot be used to make predictions.
#Because there is still self-correction in the residues. Let's refine the model by modeling the residues 
#Now we need to look at the seasonal component 

```{r}
dltraf_12 <- diff(dltraf_1, 12)
kpss.test(dltraf_12) #stationnary, we accept H0
dltraf_6 <- diff(dltraf_1, 6)
plot(dltraf_6)
kpss.test(dltraf_6)

```


```{r}
plot(dltraf_12)
par(mfrow=c(1,1))
acf(ts(dltraf_12, frequency=1), lag.max = 40)
pacf(ts(dltraf_12, frequency=1), lag.max = 40)
```
## STEP 2 : Indeitification (p,d,q,P,D,Q)

#first estimation 
q=1 #MA because for k>(q=1), acf(ts) = 0 (taking ac count of the saisonality)
p=2 #AR because for k>(q=2), pacf(ts) = 0 (taking ac count of the saisonality)
d=1 #we removed trend by taking the diff or order 1
Q=1 #MA because some significant coefficients in the ACF remain at order 12 (but not at order 24)
P=2 #AR because some significant coefficients in the PACF remain at order 12 & at order 24
D=1 #we removed trend by taking the diff or order 1

```{r}
mod1 <- arima(ltraf, c(2, 1, 1),seasonal = list(order = c(1, 1, 1), period = 12), method='ML')
mod1
```





```{r}
fit1 <- fitted(mod1)
plot.ts(cbind(ltraf,fit1),plot.type='single',col=c('black','red'))
```

```{r}
mod1$coef
mod1$var.coef
```
```{r}
tstat <- mod1$coef/sqrt(diag(mod1$var.coef))
tstat  
```
```{r}
pvalue <- 2*(1-pnorm(abs(tstat)))
pvalue 

# pvalue<5% for sma1. The coefficients assoc

```


### Remove ar1
```{r}
mod1 <- arima(ltraf, c(0, 1, 1),seasonal = list(order = c(1, 1, 1), period = 12), method='ML')
mod1
```

### Check p-value 2

```{r}
mod1$coef
mod1$var.coef
```
```{r}
tstat <- mod1$coef/sqrt(diag(mod1$var.coef))
tstat  
```
```{r}
pvalue <- 2*(1-pnorm(abs(tstat)))
pvalue 

# pvalue<5% for sma1. The coefficients assoc

```

### We remove sar1

```{r}
mod1 <- arima(ltraf, c(0, 1, 1),seasonal = list(order = c(0, 1, 1), period = 12), method='ML')
mod1
```

### We check p-value 3
```{r}
mod1$coef
mod1$var.coef
```
```{r}
tstat <- mod1$coef/sqrt(diag(mod1$var.coef))
tstat  
```

```{r}
pvalue <- 2*(1-pnorm(abs(tstat)))
pvalue
```
#We check with auto arima final coefficient 
#Test auto.arima
```{r}
auto.arima(ltraf,stationary =FALSE, seasonal = TRUE,trace=TRUE)
mod1<- arima(ltraf, c(0, 1, 1),seasonal = list(order = c(0, 1, 1), period = 12), method='ML')
mod1
fit1 <- fitted(mod1)
plot.ts(cbind(ltraf,fit1),plot.type='single',col=c('black','red'))
```




```{r}
mod1$coef
mod1$var.coef

tstat <- mod1$coef/sqrt(diag(mod1$var.coef))
tstat 
pvalue <- 2*(1-pnorm(abs(tstat)))
pvalue
```
#We have the same coef
#We follow to the residuals analysis

## Residuals analysis ## 
#Difference between fit data and raw data
#every time there is a pic explain that the model has not fitted well the data

```{r}
res1<- mod1$residuals
plot(res1)
```
# It is what we expect is that residuals are close to 0 as possible
# We want to check the white noise assumption and the normality of the residuals (gaussian WN)
# Autocorrelation of the residuals (White noise assumption) 

```{r}
acf(ts(res1, frequency=1))
```
#what we expect is that all the coefficients are none significant, that is the deal with white noise
#All coefficient are none significant

```{r}
pacf(ts(res1, frequency=1),lag.max = 60)
```


# Ljung-box test to check for the significance of ACF
```{r}
Box.test(res1,lag=20,type="Ljung-Box")
```
#pvalue = 59% larger than 5% so we accept H0: white noise assumption

#Normality assumption
#standardized residuals (residuals divided by the standard deviation)
```{r}
res_stand <- res1/sqrt(mod1$sigma2)
summary(res_stand)
```
#if the normality assumption is satisfied, the standardized residuals should 
#lie in between -2 and 2 (with 95% of chance)
```{r}
plot(res_stand) 
abline(a=2,b=0,col="red")
abline(a=-2,b=0,col="red")
```
#It should be 95% of chance but here obviously there is a big gap before 1990 
#(adding a dummy variableto fit it)
#We can identify one outlier in the dataset
#it corresponds to the lowest value of the residuals

#######
#QQ-plot
#######
```{r}
qqnorm(res1)
qqline(res1)
```

```{r}
#######
#Shapiro-test
#######
shapiro.test(1)
library(tseries)
jarque.bera.test(res1)
```
# pvalue <<<< 5%, we reject normality (H0 : normal, H1 non normal)
#constance variance
#with a plot
#if the variance is constant, the variance should not depend on the difference of the date
# Homoscedaticity 
```{r}
sq.res1 <- (res1)^2
acf(ts(sq.res1,frequency=1),lag.max = 60)
```

```{r}
#library(TSA)
Htest <- McLeod.Li.test(mod1, plot=FALSE)
Htest
```


### Dummy Variable
SARIMAX 

#Identify dummies

```{r}
plot(res_stand) 
abline(a=2,b=0,col="red")
abline(a=-2,b=0,col="red")
```
We can identify outliers in our dataset
2009 : 1 min , 2011 : 1 max , 2014 : 1 min and max



```{r}
min(res_stand)
max(res_stand)
```
We decide to cut at [-3,3]


```{r}
out <- which(res_stand < -3 | res_stand > 3)
out

#split all outliers into 4 groups to create 4 dummies 

out1 <- out[1]
out1

out2 <- out[2]
out2

out3 <- out[3]
out3

out4 <- out[4]
out4


```

It corresponds to the observations 35, 65, 100,11 

With the zoo package we associate with this observation the date. 
```{r}
library(zoo)
```

```{r}
index(res_stand)[out1] #date of the outliers novembre 2008, Mai 2011, Mars 2014, Avril 2014 
res_stand[out1] # value of the outlier. 
```

# We create the dummmies
#We split all outliers into 4 groups to create 4 dummies 
```{r}
#first dummy
Lax_data$dum_1 <- 0
Lax_data$dum_1[out1] <- 1

#second dummy
Lax_data$dum_2 <- 0
Lax_data$dum_2[out2] <- 1

#third dummy
Lax_data$dum_3 <- 0
Lax_data$dum_3[out3] <- 1

#fourth dummy
Lax_data$dum_4 <- 0
Lax_data$dum_4[out4] <- 1

```


```{r}
mod2 <- arima(ltraf, c(0, 1, 1),seasonal = list(order = c(0, 1, 1), period = 12), 
              method='ML',xreg=cbind(unlist(Lax_data$dum_1), 
                                     unlist(Lax_data$dum_2),
                                     unlist(Lax_data$dum_3),
                                     unlist(Lax_data$dum_4)
                                     ))
mod2
mod2$residuals
```

xreg : additional variable I want to take into account

By adding xreg, we will be able to control the misfit at the years written below. We have even gain small AIC ! 


Plot of the fitted value...
```{r}
fit2 <- fitted(mod2)
plot.ts(cbind(ltraf,fit2),plot.type='single',col=c('black','red'))
```


### Residuals analysis

```{r}
res2<- mod2$residuals
plot(res2)
```
# what we expect is that residuals are close to 0 as possible
```{r}
res_stand_2 <- res2/sqrt(mod2$sigma2)
summary(res_stand_2)

```

```{r}
shapiro.test(res2) #check the normality
```


```{r}
plot(res_stand_2) 
abline(a=2,b=0,col="red")
abline(a=-2,b=0,col="red")
```
```{r}
qqnorm(res2)
qqline(res2)
```

```{r}
shapiro.test(res2)
```
```{r}

acf(ts(res2, frequency=1))
pacf(ts(res2, frequency=1),lag.max = 60)

```


```{r}
# Ljung-box test to check for the significance of ACF
Box.test(res2,lag=20,type="Ljung-Box")
#pvalue = 59% larger than 5% so we accept H0: white noise assumption
```





