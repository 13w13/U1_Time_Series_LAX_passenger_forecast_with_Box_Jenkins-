---
title: "LAX_passenger_prediction_with_Box_Jenkins"
author: "Edgar Julien, Antoine Settelen, Simon Weiss"
date: "2020-10-20"
output:
  html_document :
   keep_md: true
---

# Navigation {.tabset .tabset-fade .tabset-pills}

Suggested outline : 


1. Description of the time series

2. Stationarity: Is your series stationary? Include graphs and tables to justify your analysis (acf, pacf, graph of the series). Explain how you transformed the data in order to reach stationarity (log transformation, differentiation with different orders).

3. Apply Box-Jenkins methodology to build an ARMA process on the stationary data. a. Identify the orders p and q using the ACF and PACF. b. Comment the significance of the coefficients. If necessary, modify the model until all the coefficients are significant. c. Residual diagnostic: check for the normality, the non autocorrelation assumption and the homoscedasticity of the residuals. d. Comment the information criteria values (AIC, SBC) to select the best model.

4. Validate the model with an in-sample and out-of-sample analysis and do a forecast for the next three periods.

## 1. Description of the time series

```{r}
library(magrittr)
library(data.table)
library(forecast)
library(TSA)
```

### 1.1 Load the data
```{r}
#TlseTraf <- read.csv2('TlseAirport2019.csv')
#View(TlseTraf)
#traf <- ts(TlseTraf[,3], start=c(1982,1), frequency=12)

Lax_init <- read.csv("data/los-angeles-international-airport-passenger-traffic-by-terminal.csv")
Lax_init[,2]<-Lax_init[,2] %>% as.Date()
Lax_init$year<-as.numeric(format(Lax_init[,2],'%Y'))
Lax_init$months<-months(Lax_init[,2])
Lax_init %>% setDT

Lax_data=Lax_init[, lapply(.SD, sum, na.rm=TRUE), by=list(year,months),.SDcols=c("Passenger_Count")]
#verify results

sum(Lax_init[months=="janvier"&year==2006][,6])
Lax_data[1]

```

```{r}
traf <- ts(Lax_data[,3], start=c(2006,1), frequency=12)
plot.ts(traf) #presence d'un cycle, d'une tendance, et l'amplitude du cycle croit avec le temps
#plot.ts(traf, xlim=(c(2000,2010)))

```

```{r}
acf(ts(traf, frequency=1))
pacf(ts(traf, frequency=1))
```


```{r}
#prenons le log 
ltraf <- log(traf)
#class(ltraf)

```

```{r}
plot.ts(ltraf)
#CSQ : l'amplitude du cycle est devenue stable
```


```{r}
acf(ts(ltraf, frequency=1))
#toujours pr?sence de tendance (see acf) et d'une composante saisonniere
#la tendance semble d?terministe, lin?aire et non stochastique
#serie non stationnaire
pacf(ts(ltraf, frequency=1))
```


```{r}
#diff?rention la serie une fois 
dltraf_1 <- diff(ltraf, 1)
plot(dltraf_1)
acf(ts(dltraf_1, frequency=1))
#la composante de la tendance semble avoir disparu, pas la saisonnalit? (toujours pr?sente)
pacf(ts(dltraf_1, frequency=1), lag.max = 60)
```
```{r}
#une regression lineaire de degre 1 semble donc effectivement suffisante 
#cependant, on ne peut pas utiliser le modele lm1 pour effectuer des previsions
#car il reste de l'autocorr?lation dans les r?sidus. Affinons le mod?le en mod?lisation les r?sidus 
```

```{r}
#Il faut maintenant ?tudier la composante saisonniere 
dltraf_12 <- diff(dltraf_1, 12)

kpss.test(dltraf_12) #stationnary, we accept H0

dltraf_6 <- diff(dltraf_1, 6)
plot(dltraf_6)
kpss.test(dltraf_6)

```


```{r}
plot(dltraf_12)
par(mfrow=c(1,1))
acf(ts(dltraf_12, frequency=1), lag.max = 40)
pacf(ts(dltraf_12, frequency=1), lag.max = 40)

#saisonnalit? d?terministe ou stochastique ??
```
## STEP 2 : Indeitification (p,d,q,P,D,Q)

#first estimation 
q=1 #MA because for k>(q=1), acf(ts) = 0 (taking ac count of the saisonality)
p=2 #AR because for k>(q=2), pacf(ts) = 0 (taking ac count of the saisonality)
d=1 #we removed trend by taking the diff or order 1
Q=1 #MA because some significant coefficients in the ACF remain at order 12 (but not at order 24)
P=2 #AR because some significant coefficients in the PACF remain at order 12 & at order 24
D=1 #we removed trend by taking the diff or order 1

```{r}
mod1 <- arima(ltraf, c(2, 1, 1),seasonal = list(order = c(1, 1, 1), period = 12), method='ML')
mod1
```





```{r}
fit1 <- fitted(mod1)
plot.ts(cbind(ltraf,fit1),plot.type='single',col=c('black','red'))
```
#graphiquement, c'est pas mal 
#attendons la suite pour une comparaison des mod?les ? travers l'AIC

```{r}
mod1$coef
mod1$var.coef
```
```{r}
tstat <- mod1$coef/sqrt(diag(mod1$var.coef))
tstat  
```
```{r}
pvalue <- 2*(1-pnorm(abs(tstat)))
pvalue 

# pvalue<5% for sma1. The coefficients assoc

```


### Remove ar1
```{r}
mod1 <- arima(ltraf, c(0, 1, 1),seasonal = list(order = c(1, 1, 1), period = 12), method='ML')
mod1
```

### Check p-value 2

```{r}
mod1$coef
mod1$var.coef
```
```{r}
tstat <- mod1$coef/sqrt(diag(mod1$var.coef))
tstat  
```
```{r}
pvalue <- 2*(1-pnorm(abs(tstat)))
pvalue 

# pvalue<5% for sma1. The coefficients assoc

```





### Remove sar1

```{r}
mod1 <- arima(ltraf, c(0, 1, 1),seasonal = list(order = c(0, 1, 1), period = 12), method='ML')
mod1
```
```

### Check p-value 3
```{r}
mod1$coef
mod1$var.coef
```
```{r}
tstat <- mod1$coef/sqrt(diag(mod1$var.coef))
tstat  
```

```{r}
pvalue <- 2*(1-pnorm(abs(tstat)))
pvalue
```

OK

We check with auto arima final coef 
#Test auto.arima
```{r}
auto.arima(ltraf,stationary =FALSE, seasonal = TRUE,trace=TRUE)
mod1<- arima(ltraf, c(0, 1, 1),seasonal = list(order = c(0, 1, 1), period = 12), method='ML')
mod1
fit1 <- fitted(mod1)
plot.ts(cbind(ltraf,fit1),plot.type='single',col=c('black','red'))
```




```{r}
mod1$coef
mod1$var.coef

tstat <- mod1$coef/sqrt(diag(mod1$var.coef))
tstat 
pvalue <- 2*(1-pnorm(abs(tstat)))
pvalue
```


Ok we have the same coef
Let go to the residuals analysis

```{r}
## Residuals analysis ## 
#difference between fit data and raw data
#every time there is a pic explain that the model has not fitted well the data
res2<- mod1$residuals
plot(res2)
# what we expect is that residuals are close to 0 as possible
```

```{r}
#we want to check the white noise assumption and the normality of the residuals (gaussian WN)
# Autocorrelation of the residuals (White noise assumption) 
acf(ts(res2, frequency=1))
#what we expect is that all the coefficients are none significant, that is the deal with white noise
#All coefficient are none significant
```



```{r}
pacf(ts(res1, frequency=1),lag.max = 60)
#coefficient lag 4 is nearly significant
```



```{r}
# Ljung-box test to check for the significance of ACF
Box.test(res1,lag=20,type="Ljung-Box")
#pvalue = 59% larger than 5% so we accept H0: white noise assumption
```
```{r}
#Normality assumption
#standardized residuals (residuals divided by the standard deviation)
res_stand <- res1/sqrt(mod1$sigma2)
summary(res_stand)
#if the normality assumption is satisfied, the standardized residuals should 
#lie in between -2 and 2 (with 95% of chance)
```
```{r}
plot(res_stand) 
abline(a=2,b=0,col="red")
abline(a=-2,b=0,col="red")
#it should be 95% of chance but here obviously there is a big gap before 1990 (adding a dummy variable 
#to fit it)
#we can identify one outlier in the dataset
#it corresponds to the lowest value of the residuals
```
```{r}
#######
#QQ-plot
#######
qqnorm(res_test)
qqline(res_test)
```

```{r}
#######
#Shapiro-test
#######
shapiro.test(res1)
# pvalue <<<< 5%, we reject normality (H0 : normal, H1 non normal)
library(tseries)
jarque.bera.test(res1)

```
```{r}
#constance variance
#with a plot
#if the variance is constant, the variance should not depend on the difference of the date
# Homoscedaticity 
sq.res <- (res1)^2
acf(ts(sq.res,frequency=1),lag.max = 60)

```



```{r}
#library(TSA)
Htest <- McLeod.Li.test(mod2, plot=FALSE)
Htest
```


### Dummy Variable
SARIMAX 

#Identify dummies

```{r}
plot(res_stand) 
abline(a=2,b=0,col="red")
abline(a=-2,b=0,col="red")
```
We can identify outliers in our dataset
2009 : 1 min , 2011 : 1 max , 2014 : 1 min and max



```{r}
min(res_stand)
max(res_stand)
```
We decide to cut at [-3,3]


```{r}
out1 <- which(res_stand < -3 | res_stand > 3)
out1
```

It corresponds to the observations 35, 65, 100,11 

With the zoo package we associate with this observation the date. 
```{r}
library(zoo)
```

```{r}
index(res_stand)[out1] #date of the outliers novembre 2008, Mai 2011, Mars 2014, Avril 2014 
res_stand[out1] # value of the outlier. 
```


# Create the dummmies
```{r}
Lax_data$dum_1 <- 0
Lax_data$dum_1[out1] <- 1

?arimax

Lax_data %>% as.data.frame

```


```{r}
mod2 <- arima(ltraf, c(0, 1, 1),seasonal = list(order = c(0, 1, 1), period = 12), method='ML',xreg=Lax_data$dum_1)
mod2
mod1
```

xreg : additional variable I want to take into account

By adding xreg, we will be able to control the misfit at the years written below. We have even gain small AIC ! 


Plot of the fitted value...
```{r}
fit2 <- fitted(mod2)


plot.ts(cbind(ltraf,fit2),plot.type='single',col=c('black','red'))
```


### Residuals analysis



```{r}
res2<- mod2$residuals
plot(res2)
# what we expect is that residuals are close to 0 as possible
```

```{r}

acf(ts(res2, frequency=1))
pacf(ts(res2, frequency=1),lag.max = 60)

```


```{r}
# Ljung-box test to check for the significance of ACF
Box.test(res2,lag=20,type="Ljung-Box")
#pvalue = 59% larger than 5% so we accept H0: white noise assumption
```
```{r}
#Normality assumption
#standardized residuals (residuals divided by the standard deviation)
res_stand_2 <- res2/sqrt(mod2$sigma2)
summary(res_stand_2)
#if the normality assumption is satisfied, the standardized residuals should 
#lie in between -2 and 2 (with 95% of chance)
```
```{r}
plot(res_stand_2) 
abline(a=2,b=0,col="red")
abline(a=-2,b=0,col="red")


#it should be 95% of chance but here obviously there is a big gap before 1990 (adding a dummy variable 
#to fit it)
#we can identify one outlier in the dataset
#it corresponds to the lowest value of the residuals
```
```{r}
#######
#QQ-plot
#######
qqnorm(res_test_2)
qqline(res_test)
```

```{r}
#######
#Shapiro-test
#######
shapiro.test(res2)
# pvalue <<<< 5%, we reject normality (H0 : normal, H1 non normal)
library(tseries)
jarque.bera.test(res2)

```
```{r}
#constance variance
#with a plot
#if the variance is constant, the variance should not depend on the difference of the date
# Homoscedaticity 
sq.res <- (res2)^2
acf(ts(sq.res,frequency=1),lag.max = 60)

```





